{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51367d3b",
   "metadata": {},
   "source": [
    "# SECTION 1 : IMPORTS AND CONFIGURATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112a62da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and configuration loaded\n",
      "  - Using 600000 samples\n",
      "  - Learning rate: 0.01\n",
      "  - Epochs: 300\n",
      "  - Features: ['age', 'study_hours', 'class_attendance', 'sleep_hours']\n",
      "  - Target: exam_score\n",
      "  - Data path: data/train.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "\n",
    "# Core numerical computing library - handles arrays, matrix operations, math\n",
    "import numpy as np\n",
    "\n",
    "# Data manipulation and loading CSV files\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization library for plotting loss curves and results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make plots appear in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Random seed for reproducibility - same random numbers every run\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Number of training samples to use (subset of full 630k for faster learning)\n",
    "N_SAMPLES = 600000\n",
    "\n",
    "# Learning rate (eta) - controls step size in gradient descent\n",
    "# Too large = overshooting, too small = slow convergence\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Number of passes through the entire dataset during training\n",
    "N_EPOCHS = 300\n",
    "\n",
    "# Path to training data\n",
    "DATA_PATH = \"data/train.csv\"\n",
    "\n",
    "# Features to use (numeric only for now, we'll skip categoricals)\n",
    "# We avoid 'id' because it has no predictive power\n",
    "NUMERIC_FEATURES = [\n",
    "    'age',\n",
    "    'study_hours', \n",
    "    'class_attendance',\n",
    "    'sleep_hours'\n",
    "]\n",
    "\n",
    "# Target column name\n",
    "TARGET = 'exam_score'\n",
    "\n",
    "# Set numpy random seed for reproducible weight initialization\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Imports and configuration loaded\")\n",
    "print(f\"  - Using {N_SAMPLES} samples\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Epochs: {N_EPOCHS}\")\n",
    "print(f\"  - Features: {NUMERIC_FEATURES}\")\n",
    "print(f\"  - Target: {TARGET}\")\n",
    "print(f\"  - Data path: {DATA_PATH}\")\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e51676",
   "metadata": {},
   "source": [
    "# SECTION 2 : DATA LOADING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c145b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from data/train.csv: 600000 samples, 4 features\n",
      "Target range: [19.60, 100.00]\n"
     ]
    }
   ],
   "source": [
    "def load_data(path: str, features: list, target: str, n_samples: int):\n",
    "    \"\"\"\n",
    "    Load training data from CSV and prepare features and target.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the CSV file\n",
    "        features: List of feature column names to use\n",
    "        target: Name of the target column\n",
    "        n_samples: Number of samples to load (None = all data)\n",
    "    \n",
    "    Returns:\n",
    "        X: Feature matrix as numpy array, shape (n_samples, n_features)\n",
    "        y: Target vector as numpy array, shape (n_samples,)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(path, nrows=n_samples) # Load data from CSV into dataframe using pandas\n",
    "\n",
    "    x_df = df[features]  # Select only the specified feature columns\n",
    "\n",
    "    y_df = df[target]    # Select the target column as a pandas Series\n",
    "\n",
    "    x = x_df.to_numpy()  # Convert feature dataframe to numpy array\n",
    "\n",
    "    y = y_df.to_numpy()  # Convert target series to numpy array\n",
    "    print(f\"Data loaded from {path}: {x.shape[0]} samples, {x.shape[1]} features\")\n",
    "    print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "    return x, y\n",
    "x_train, y_train = load_data(DATA_PATH, NUMERIC_FEATURES, TARGET, N_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf27ed",
   "metadata": {},
   "source": [
    "# SECTION 3: FEATURE NORMALIZATION (Standardization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b256c0",
   "metadata": {},
   "source": [
    "\n",
    "## Why We Need It\n",
    "\n",
    "**Problem:** Different features have different scales:\n",
    "\n",
    "- `age`: 17-24 (small numbers)\n",
    "\n",
    "- `study_hours`: 0-8 (small numbers)\n",
    "\n",
    "- `class_attendance`: 0-100 (large numbers)\n",
    "\n",
    "**What happens without normalization:**\n",
    "\n",
    "Gradient descent updates weights using:\n",
    "\n",
    "$w = w - η * gradient$\n",
    "\n",
    "### Breaking Down each piece:\n",
    "\n",
    "- `w` (on the left side): The new weight value after this update\n",
    "\n",
    "- `w` (on the right side): The current weight value before this update\n",
    "\n",
    "- `η` (eta, also written as n or learning_rate): How big a step you take\n",
    "\n",
    "**Example**: 0.01 means \"move 1% in the gradient direction\" gradient:\n",
    "\n",
    "*This is ∂J/∂w from Section 3(mathematical_concepts_involved.md)*\n",
    "\n",
    "- Tells you \"which direction increases loss\"\n",
    "\n",
    "- Formula we derived: gradient = `-1/N * Σ(e_i * x_i)`\n",
    "\n",
    "*the `-` (minus sign): Move opposite to the gradient direction*\n",
    "\n",
    "**Why?**\n",
    "\n",
    "- Gradient points uphill (toward higher loss), we want downhill (toward lower loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If `class_attendance` has values ~100 and `age` has values ~20:\n",
    "\n",
    "- Gradients for `class_attendance` weight will be ~5x larger\n",
    "\n",
    "- Model will take huge steps for attendance, tiny steps for age\n",
    "\n",
    "- Training becomes unstable, slow, or diverges\n",
    "\n",
    "**Example (without normalization):**\n",
    "\n",
    "Student 1: [age=20, study=5, attendance=90, sleep=7]\n",
    "\n",
    "Student 2: [age=21, study=6, attendance=45, sleep=8]\n",
    "\n",
    "The attendance column dominates because its values are 10x larger than other features.\n",
    "\n",
    "---\n",
    "\n",
    "## The Solution: Standardization\n",
    "\n",
    "Transform each feature to have:\n",
    "\n",
    "- **Mean = 0** (centered around zero)\n",
    "\n",
    "- **Standard deviation = 1** (similar spread)\n",
    "\n",
    "**Formula for each feature column:**\n",
    "\n",
    "$x_{norm} = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "\n",
    "where:\n",
    "- $μ$ = mean of that feature across all samples\n",
    "\n",
    "- $σ$ = standard deviation of that feature\n",
    "\n",
    "**After normalization:**\n",
    "\n",
    "Student 1: [age=-0.5, study=0.2, attendance=1.1, sleep=-0.3]\n",
    "\n",
    "Student 2: [age=0.3, study=0.8, attendance=-0.9, sleep=0.5]\n",
    "\n",
    "Now all features are on similar scales → gradient descent converges faster and more reliably.\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works Step-by-Step\n",
    "\n",
    "**Given:** X = matrix with shape (10000, 4)\n",
    "\n",
    "**Step 1:** Compute mean of each column\n",
    "```\n",
    "mean = np.mean(X, axis=0)  # Shape: (4,)\n",
    "```\n",
    "**Result:** [mean_age, mean_study, mean_attendance, mean_sleep]\n",
    "\n",
    "**Step 2:** Compute std of each column\n",
    "```\n",
    "std = np.std(X, axis=0)  # Shape: (4,)\n",
    "```\n",
    "**Result:** [std_age, std_study, std_attendance, std_sleep]\n",
    "\n",
    "**Step 3:** Apply formula to entire matrix\n",
    "\n",
    "```\n",
    "X_norm = (X - mean) / std\n",
    "```\n",
    "\n",
    "NumPy broadcasting automatically:\n",
    "\n",
    "- Subtracts mean from every row\n",
    "\n",
    "- Divides by std for every row\n",
    "\n",
    "---\n",
    "\n",
    "## Concrete Example\n",
    "\n",
    "Original feature column (age):\n",
    "[20, 21, 19, 22, 20]\n",
    "\n",
    "**Step 1:** mean = 20.4\n",
    "\n",
    "**Step 2:** std = 1.02\n",
    "\n",
    "**Step 3:** Normalize each value:\n",
    "(20 - 20.4) / 1.02 = -0.39\n",
    "(21 - 20.4) / 1.02 =  0.59\n",
    "(19 - 20.4) / 1.02 = -1.37\n",
    "(22 - 20.4) / 1.02 =  1.57\n",
    "(20 - 20.4) / 1.02 = -0.39\n",
    "\n",
    "New column mean ≈ 0, std ≈ 1 ✓\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points\n",
    "\n",
    "1. We normalize **only the features (X)**, NOT the target (y)\n",
    "\n",
    "2. We save `mean` and `std` to normalize test data the same way later\n",
    "\n",
    "3. This is called \"standardization\" (mean=0, std=1)\n",
    "\n",
    "4. Another method is \"min-max scaling\" (range 0-1), but standardization works better for gradient descent\n",
    "\n",
    "5. After normalization, all features contribute equally to gradient updates\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Helps Gradient Descent\n",
    "\n",
    "- Loss surface becomes more spherical (not elongated)\n",
    "\n",
    "- Learning rate can be the same for all features\n",
    "\n",
    "- Converges in fewer iterations\n",
    "\n",
    "- More stable training (less chance of divergence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d384bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalized\n",
      "  Mean before: [20.54638333  4.00184421 71.98173312  7.07274543]\n",
      "  Mean after: [-6.22340697e-16  8.84388858e-17  7.55306928e-16  1.57077314e-16]\n",
      "  Std before: [ 2.25996503  2.3594731  17.42940411  1.74489416]\n",
      "  Std after: [1. 1. 1. 1.]\n",
      "\n",
      " X_train_norm shape: (600000, 4) \n",
      "First 3 normalised samples:\n",
      "[[ 0.20071845  1.65636802  1.5386795  -1.24520185]\n",
      " [-1.12673573  0.40185065  1.30918227 -1.35982198]\n",
      " [-0.24176628  0.28741832  1.18295879 -0.72941125]]\n"
     ]
    }
   ],
   "source": [
    "def normalize_features(X: np.ndarray):\n",
    "    \"\"\"\n",
    "    Normalize features to have mean=0 and std=1 (standardization).\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features)\n",
    "    \n",
    "    Returns:\n",
    "        X_norm: Normalized feature matrix, same shape as X\n",
    "        mean: Mean of each feature (for later use), shape (n_features,)\n",
    "        std: Std of each feature (for later use), shape (n_features,)\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0) # Mean of each feature : computes mean down the rows (one per column) \n",
    "\n",
    "    std = np.std(X, axis=0)   # computes standard deviation of each column \n",
    "\n",
    "    X_norm = (X - mean) / std  # Standardization formula\n",
    "\n",
    "    print(\"Features normalized\")\n",
    "    print(f\"  Mean before: {X.mean(axis=0)}\")\n",
    "    print(f\"  Mean after: {X_norm.mean(axis=0)}\")\n",
    "    print(f\"  Std before: {X.std(axis=0)}\")\n",
    "    print(f\"  Std after: {X_norm.std(axis=0)}\")\n",
    "\n",
    "    return X_norm, mean, std\n",
    "\n",
    "x_train_norm, x_mean, x_std = normalize_features(x_train)\n",
    "\n",
    "print(f'\\n X_train_norm shape: {x_train_norm.shape} ')\n",
    "print(f\"First 3 normalised samples:\")\n",
    "print(x_train_norm[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c29c5",
   "metadata": {},
   "source": [
    "# SECTION 4: CORE FUNCTIONS (Prediction, Loss, Gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5197b3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized parameters:\n",
      "  w shape: (4,), values: [ 0.00496714 -0.00138264  0.00647689  0.0152303 ]\n",
      "  b: 0.0\n",
      "\n",
      "Predictions computed:\n",
      "  Shape: (600000,)\n",
      "  First 5: [-0.01029211 -0.01838334 -0.00504555  0.00013202  0.03085782]\n",
      "\n",
      "Loss computed: 4264.6900\n",
      "\n",
      "Gradients computed:\n",
      "  grad_w shape: (4,), values: [ -0.1945173  -14.41281245  -6.81075388  -3.15781533]\n",
      "  grad_b: -62.5077\n"
     ]
    }
   ],
   "source": [
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Compute predictions using linear model: y_hat = X @ w + b\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features)\n",
    "        w: Weight vector, shape (n_features,)\n",
    "        b: Bias scalar\n",
    "    \n",
    "    Returns:\n",
    "        y_pred: Predictions, shape (n_samples,)\n",
    "    \"\"\" \n",
    "    y_pred = X @ w + b  # Matrix-vector multiplication plus bias\n",
    "    return y_pred\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error loss.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values, shape (n_samples,)\n",
    "        y_pred: Predicted values, shape (n_samples,)\n",
    "    \n",
    "    Returns:\n",
    "        loss: MSE scalar value\n",
    "    \"\"\"\n",
    "    errors = y_true - y_pred  # Compute errors\n",
    "\n",
    "    loss = np.mean(errors ** 2)  # Mean Squared Error formula\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_gradients(X, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute gradients of MSE w.r.t. weights and bias.\n",
    "    \n",
    "    From Section 3 theory:\n",
    "        grad_w = -1/N * sum(e_i * x_i)\n",
    "        grad_b = -1/N * sum(e_i)\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features)\n",
    "        y_true: Actual target values, shape (n_samples,)\n",
    "        y_pred: Predicted values, shape (n_samples,)\n",
    "    \n",
    "    Returns:\n",
    "        grad_w: Gradient w.r.t. weights, shape (n_features,)\n",
    "        grad_b: Gradient w.r.t. bias, scalar\n",
    "    \"\"\"\n",
    "    N = X.shape[0]  # Number of samples\n",
    "\n",
    "    errors = y_true - y_pred  # Compute errors\n",
    "\n",
    "    grad_w = (-1 / N) * (X.T @ errors)  # Gradient w.r.t. weights\n",
    "\n",
    "    grad_b = (-1 / N) * np.sum(errors)   # Gradient w.r.t. bias\n",
    "\n",
    "    return grad_w, grad_b\n",
    "\n",
    "\n",
    "# TEST : Initialize random parameters and test functions\n",
    "\n",
    "\n",
    "# Initialize small random weights and bias\n",
    "n_features = x_train_norm.shape[1]  # Should be 4\n",
    "w_test = np.random.randn(n_features) * 0.01\n",
    "b_test = 0.0\n",
    "\n",
    "print(f\"Initialized parameters:\")\n",
    "print(f\"  w shape: {w_test.shape}, values: {w_test}\")\n",
    "print(f\"  b: {b_test}\")\n",
    "\n",
    "# TO-DO : Test predict function\n",
    "y_pred_test = predict(x_train_norm, w_test, b_test)\n",
    "\n",
    "print(f\"\\nPredictions computed:\")\n",
    "print(f\"  Shape: {y_pred_test.shape}\")\n",
    "print(f\"  First 5: {y_pred_test[:5]}\")\n",
    "\n",
    "# TO-DO : Test compute_loss function\n",
    "loss_test = compute_loss(y_train, y_pred_test)\n",
    "\n",
    "print(f\"\\nLoss computed: {loss_test:.4f}\")\n",
    "\n",
    "# TO-DO : Test compute_gradients function\n",
    "grad_w_test, grad_b_test = compute_gradients(x_train_norm, y_train, y_pred_test)\n",
    "\n",
    "print(f\"\\nGradients computed:\")\n",
    "print(f\"  grad_w shape: {grad_w_test.shape}, values: {grad_w_test}\")\n",
    "print(f\"  grad_b: {grad_b_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40c65a",
   "metadata": {},
   "source": [
    "\n",
    "## Complete Explanation of Test Section\n",
    "\n",
    "### Part 1: Initialize Random Parameters\n",
    "\n",
    "```python\n",
    "n_features = x_train_norm.shape[1]  # Should be 4\n",
    "```\n",
    "\n",
    "- Gets the number of columns in your feature matrix\n",
    "\n",
    "- `x_train_norm.shape = (10000, 4)` → `shape[1] = 4`\n",
    "\n",
    "- We have 4 features: age, study_hours, class_attendance, sleep_hours\n",
    "\n",
    "```python\n",
    "w_test = np.random.randn(n_features) * 0.01\n",
    "```\n",
    "\n",
    "- Creates 4 random weights (one per feature), very small values\n",
    "\n",
    "- `np.random.randn(4)` gives random numbers from normal distribution (mean=0, std=1)\n",
    "\n",
    "- `* 0.01` scales them down to tiny values like 0.005\n",
    "\n",
    "- **Why small?** So initial predictions aren't crazy far from reality\n",
    "\n",
    "```python\n",
    "b_test = 0.0\n",
    "```\n",
    "\n",
    "- Initialize bias to zero (common practice)\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "w shape: (4,), values: [0.00496714 -0.00138264 0.00647689 0.0152303]\n",
    "```\n",
    "\n",
    "- 4 weights, each around ±0.01\n",
    "\n",
    "- These are random, so your values will differ each run\n",
    "\n",
    "***\n",
    "\n",
    "### Part 2: Test Prediction Function\n",
    "\n",
    "```python\n",
    "y_pred_test = predict(x_train_norm, w_test, b_test)\n",
    "```\n",
    "\n",
    "**What happens inside `predict`:**\n",
    "\n",
    "1. Takes your 10,000 students (rows) with 4 features each\n",
    "\n",
    "2. For each student, computes: \n",
    "   ```\n",
    "   prediction = w*age + w[1]*study + w[2]*attendance + w[3]*sleep + b\n",
    "   ```\n",
    "\n",
    "3. Returns 10,000 predictions (one per student)\n",
    "\n",
    "**Example for first student:**\n",
    "```\n",
    "Student 1: [0.204, 1.659, 1.531, -1.232] (normalized features)\n",
    "\n",
    "Prediction = 0.005*0.204 + (-0.001)*1.659 + 0.006*1.531 + 0.015*(-1.232) + 0\n",
    "           ≈ -0.010\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Shape: (10000,)\n",
    "First 5: [-0.01012532 -0.01822252 -0.00487317 0.00037149 0.03104513]\n",
    "```\n",
    "- ✓ 10,000 predictions (correct)\n",
    "\n",
    "- Values are around zero because:\n",
    "\n",
    "  - Weights are tiny (×0.01)\n",
    "  - Bias is zero\n",
    "  - Features are normalized (mean=0)\n",
    "\n",
    "- **These are terrible predictions** (exam scores should be 20-100, not near 0)\n",
    "\n",
    "- That's expected - we haven't trained yet!\n",
    "\n",
    "***\n",
    "\n",
    "### Part 3: Test Loss Function\n",
    "\n",
    "```python\n",
    "loss_test = compute_loss(y_train, y_pred_test)\n",
    "```\n",
    "\n",
    "**What happens inside `compute_loss`:**\n",
    "\n",
    "1. Computes errors for all 10,000 students:\n",
    "   ```\n",
    "   errors = actual_scores - predictions\n",
    "   errors = [57.7, 57.4, 78.9, ...] - [-0.01, -0.02, -0.005, ...]\n",
    "   errors ≈ [57.7, 57.4, 78.9, ...] (basically same as actual scores)\n",
    "   ```\n",
    "\n",
    "2. Squares each error: `[57.7², 57.4², 78.9², ...]`\n",
    "\n",
    "3. Takes average (mean)\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Loss computed: 4262.4007\n",
    "```\n",
    "\n",
    "**What this means:**\n",
    "\n",
    "- MSE = 4262.4\n",
    "\n",
    "- √4262.4 ≈ 65 points average error (RMSE)\n",
    "\n",
    "- **This is horrible** - we're off by ~65 points on average\n",
    "\n",
    "- Why? Because our random weights make predictions near 0, but actual scores are 20-100\n",
    "\n",
    "- After training, this should drop dramatically (target: <10)\n",
    "\n",
    "***\n",
    "\n",
    "### Part 4: Test Gradient Function\n",
    "\n",
    "```python\n",
    "grad_w_test, grad_b_test = compute_gradients(x_train_norm, y_train, y_pred_test)\n",
    "```\n",
    "\n",
    "**What happens inside `compute_gradients`:**\n",
    "\n",
    "1. Computes errors (same as loss function):\n",
    "   ```\n",
    "   errors = y_train - y_pred_test\n",
    "   errors ≈ [57.7, 57.4, 78.9, ...] (since predictions ≈ 0)\n",
    "   ```\n",
    "\n",
    "\n",
    "2. Computes `grad_w` (gradient for weights):\n",
    "   ```\n",
    "   grad_w = -1/10000 * X.T @ errors\n",
    "   ```\n",
    "   - For each feature, this computes: \"how correlated is this feature with the errors?\"\n",
    "\n",
    "   - Negative sign because we derived it that way in Section 3\n",
    "\n",
    "\n",
    "3. Computes `grad_b` (gradient for bias):\n",
    "   ```\n",
    "   grad_b = -1/10000 * sum(errors)\n",
    "   grad_b = -1/10000 * (57.7 + 57.4 + ... all 10k values)\n",
    "   grad_b = -average_error\n",
    "   grad_b = -62.43\n",
    "   ```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "grad_w: [-0.20, -14.56, -6.99, -3.27]\n",
    "grad_b: -62.4315\n",
    "```\n",
    "\n",
    "**What these numbers mean:**\n",
    "\n",
    "**`grad_b = -62.43`:**\n",
    "\n",
    "- Average prediction error is +62.43 points (we're predicting too low)\n",
    "\n",
    "- Gradient is negative → tells us to **increase bias** in next update\n",
    "\n",
    "- Update rule: `b = 0.0 - 0.01 * (-62.43) = 0.624`\n",
    "\n",
    "- New bias will shift all predictions up by ~0.6 points\n",
    "\n",
    "\n",
    "`grad_w[1] = -14.56` **(study_hours weight):**\n",
    "\n",
    "- Large negative gradient → increase this weight a lot\n",
    "\n",
    "- Students who study more tend to have higher errors (we're underpredicting their scores)\n",
    "\n",
    "- Update rule: `w[1] = -0.0014 - 0.01 * (-14.56) = 0.144`\n",
    "\n",
    "- New weight will make study_hours contribute more to predictions\n",
    "\n",
    "\n",
    "**`grad_w[0] = -0.20` (age weight):**\n",
    "\n",
    "- Small negative gradient → age doesn't correlate much with errors\n",
    "\n",
    "- Will get a small update\n",
    "\n",
    "***\n",
    "\n",
    "## Summary: What Just Happened\n",
    "\n",
    "| What You Did | Result | Why It Matters |\n",
    "|--------------|--------|----------------|\n",
    "| Initialize random weights | `w ≈ [0.005, -0.001, 0.006, 0.015]` | Starting point for learning |\n",
    "| Make predictions | All predictions ≈ 0 | Terrible, but expected with random weights |\n",
    "| Compute loss | MSE = 4262 (RMSE ≈ 65) | Quantifies how bad predictions are |\n",
    "| Compute gradients | `grad_w, grad_b` | Tells us **how to improve** weights |\n",
    "\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36fdc60",
   "metadata": {},
   "source": [
    "# SECTION 5: TRAINING LOOP (Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41499d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 300 epochs, learning rate=0.01\n",
      "------------------------------------------------------------\n",
      "Epoch   0/300: Loss = 4264.6415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/300: Loss = 3505.4549\n",
      "Epoch  20/300: Loss = 2885.1661\n",
      "Epoch  30/300: Loss = 2378.3486\n",
      "Epoch  40/300: Loss = 1964.2333\n",
      "Epoch  50/300: Loss = 1625.8549\n",
      "Epoch  60/300: Loss = 1349.3546\n",
      "Epoch  70/300: Loss = 1123.4110\n",
      "Epoch  80/300: Loss = 938.7754\n",
      "Epoch  90/300: Loss = 787.8918\n",
      "Epoch 100/300: Loss = 664.5871\n",
      "Epoch 110/300: Loss = 563.8179\n",
      "Epoch 120/300: Loss = 481.4636\n",
      "Epoch 130/300: Loss = 414.1574\n",
      "Epoch 140/300: Loss = 359.1483\n",
      "Epoch 150/300: Loss = 314.1886\n",
      "Epoch 160/300: Loss = 277.4416\n",
      "Epoch 170/300: Loss = 247.4063\n",
      "Epoch 180/300: Loss = 222.8565\n",
      "Epoch 190/300: Loss = 202.7897\n",
      "Epoch 200/300: Loss = 186.3871\n",
      "Epoch 210/300: Loss = 172.9792\n",
      "Epoch 220/300: Loss = 162.0191\n",
      "Epoch 230/300: Loss = 153.0597\n",
      "Epoch 240/300: Loss = 145.7356\n",
      "Epoch 250/300: Loss = 139.7483\n",
      "Epoch 260/300: Loss = 134.8536\n",
      "Epoch 270/300: Loss = 130.8521\n",
      "Epoch 280/300: Loss = 127.5808\n",
      "Epoch 290/300: Loss = 124.9063\n",
      "Epoch 299/300: Loss = 122.9191\n",
      "------------------------------------------------------------\n",
      "✓ Training complete!\n",
      "  Final loss: 122.9191\n",
      "  Initial loss: 4264.6415\n",
      "  Improvement: 4141.7224\n",
      "\n",
      "Learned parameters:\n",
      "  Weights: [ 0.07069978 13.20355199  5.44468805  2.40144017]\n",
      "  Bias: 59.4422\n"
     ]
    }
   ],
   "source": [
    "def train_gradient_descent(X, y, learning_rate, n_epochs):\n",
    "    \"\"\"\n",
    "    Train linear regression using batch gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X: Normalized feature matrix, shape (n_samples, n_features)\n",
    "        y: Target values, shape (n_samples,)\n",
    "        learning_rate: Step size (eta)\n",
    "        n_epochs: Number of full passes through the data\n",
    "    \n",
    "    Returns:\n",
    "        w: Learned weights, shape (n_features,)\n",
    "        b: Learned bias, scalar\n",
    "        loss_history: List of loss values per epoch (for plotting)\n",
    "    \"\"\"\n",
    "    N_SAMPLES, N_FEATURES = X.shape\n",
    "\n",
    "    #initialize weights and bias\n",
    "    w = np.random.randn(N_FEATURES) * 0.01\n",
    "    b = 0.0\n",
    "\n",
    "    #track loss history\n",
    "    loss_history = []   \n",
    "\n",
    "    print(f\"Starting training for {n_epochs} epochs, learning rate={learning_rate}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # main training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = predict(X, w, b)  # Forward pass: compute predictions\n",
    "\n",
    "        loss = compute_loss(y, y_pred)  # Compute loss\n",
    "        loss_history.append(loss)  # Store loss for this epoch  \n",
    "\n",
    "        grad_w , grad_b = compute_gradients(X, y, y_pred)  # Compute gradients\n",
    "\n",
    "        w -= learning_rate * grad_w  # Update weights\n",
    "        b -= learning_rate * grad_b  # Update bias\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
    "            print(f\"Epoch {epoch:3d}/{n_epochs}: Loss = {loss:.4f}\")\n",
    "        \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"✓ Training complete!\")\n",
    "    print(f\"  Final loss: {loss_history[-1]:.4f}\")\n",
    "    print(f\"  Initial loss: {loss_history[0]:.4f}\")\n",
    "    print(f\"  Improvement: {loss_history[0] - loss_history[-1]:.4f}\")\n",
    "    \n",
    "    return w, b, loss_history\n",
    "\n",
    "\n",
    "# Train the model using gradient descent\n",
    "w_learned, b_learned, loss_history = train_gradient_descent(x_train_norm, y_train, LEARNING_RATE, N_EPOCHS)\n",
    "\n",
    "print(\"\\nLearned parameters:\")\n",
    "print(f\"  Weights: {w_learned}\")        \n",
    "print(f\"  Bias: {b_learned:.4f}\")   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca8a26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Final RMSE: 11.09 points\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE TRAINING PROGRESS\n",
    "# ============================================================================\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.plot(loss_history, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Final RMSE: {np.sqrt(loss_history[-1]):.2f} points\")\n",
    "\n",
    "# ============================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
